# -*- coding: utf-8 -*-
"""classifier_model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hXq2h-MHR3HZIA49S3vrESyIu8JBmyav
"""

from transformers import BertForSequenceClassification, BertTokenizer
import torch
from transformers import (
    DistilBertForSequenceClassification,
    Trainer,
    TrainingArguments,
    BertModel,
)
from mikatools import *
import random
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score
import nltk
from transformers import CamembertTokenizer
from transformers.models.camembert.modeling_camembert import (
    CamembertForSequenceClassification,
)
from pathlib import Path
import re
import string
from langdetect import detect

seed = 42
torch.manual_seed(seed)


class IMDbDataset(torch.utils.data.Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    def __getitem__(self, idx):
        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
        item["labels"] = torch.tensor(self.labels[idx])
        return item

    def __len__(self):
        return len(self.labels)


def sent_tokenize(source):
    source = [j for i in source for j in i]

    source = [str(x) for x in source if x]

    source = [nltk.sent_tokenize(sent) for sent in source]
    source = [j for i in source for j in i]
    return source


def del_eng_sents(source):
    source_copy = []
    for sent in source:
        try:
            detection = detect(sent)
        except:
            detection = "en"
        if detection != "en":
            source_copy.append(sent)

    return source_copy


def get_traintest_data(langs):
    df = pd.read_csv(
        "data/persuasion_dialogue_dataset.csv",
        index_col=[0],
        encoding="utf-8",
        engine="python",
    )
    df_kotor1_neg = df.loc[(df.is_persuasion == False) & (df.game == "KotOR 1")]
    df_kotor2_neg = df.loc[(df.is_persuasion == False) & (df.game == "KotOR 2")]
    df_nwn_neg = df.loc[(df.is_persuasion == False) & (df.game == "NWN 1")]
    df_persuade_sents = df.loc[(df.is_persuasion == True)]
    df_kotor2_neg = df_kotor2_neg.dropna(subset=["de", "it", "es", "fr"])
    df_nwn_neg = df_nwn_neg.dropna(subset=["de", "it", "es", "fr"])
    df_persuade_sents = df_persuade_sents.dropna(subset=["de", "it", "es", "fr"])
    print(len(df_persuade_sents))
    """
    for (
        index,
        row,
    ) in (
        df_kotor2_neg.iterrows()
    ):  # = df_kotor2_neg.drop(df_kotor2_neg[np.where(detect(df_kotor2_neg.de)) == 'en'].index)
        try:
            lang = (
                detect(row["fr"])
                + detect(row["de"])
                + detect(row["it"])
                + detect(row["es"])
            )
        except:
            lang = "en"
        if "en" in lang:
            df_kotor2_neg.drop(index, inplace=True)
    for index, row in df_nwn_neg.iterrows():
        try:
            lang = (
                detect(row["fr"])
                + detect(row["de"])
                + detect(row["it"])
                + detect(row["es"])
            )
        except:
            lang = "en"
        if "en" in lang:
            df_nwn_neg.drop(index, inplace=True)
    """
    print("kotor1:", len(df_kotor1_neg))
    print("kotor2:", len(df_kotor2_neg))
    print("nwn", len(df_nwn_neg))
    print("persuade", len(df_persuade_sents))
    kotor1_neg = []
    kotor2_neg = []
    nwn_neg = []
    pos_sents = []
    if type(langs) is list:
        for lang in langs:
            kotor1_neg.append(list(set(df_kotor1_neg[lang].tolist())))
            kotor2_neg.append(list(set(df_kotor2_neg[lang].tolist())))
            nwn_neg.append(list(set(df_nwn_neg[lang].tolist())))
            pos_sents.append(list(set(df_persuade_sents[lang].tolist())))
    else:
        kotor1_neg.append(list(set(df_kotor1_neg[langs].tolist())))
        kotor2_neg.append(list(set(df_kotor2_neg[langs].tolist())))
        nwn_neg.append(list(set(df_nwn_neg[langs].tolist())))
        pos_sents.append(list(set(df_persuade_sents[langs].tolist())))

    kotor1_neg = sent_tokenize(kotor2_neg)
    kotor2_neg = sent_tokenize(kotor2_neg)
    nwn_neg = sent_tokenize(nwn_neg)
    persuade_sents = sent_tokenize(pos_sents)
    #  if type(langs) is str:
    #   if langs != 'en':
    #        kotor2_neg = del_eng_sents(kotor2_neg)
    #       nwn_neg = del_eng_sents(nwn_neg)

    pos_num = int(len(persuade_sents) * 0.7)
    pos_test = int(len(persuade_sents) * 0.3)
    # print(kotor1_neg)
    # print(kotor2_neg)
    # print(nwn_neg)
    # print(pos_sents)
    neg_sents = kotor2_neg[:pos_num] + nwn_neg[:pos_num] + kotor1_neg[:pos_num]

    pos_sents = persuade_sents[:pos_num]
    neg_sents_test = (
        kotor2_neg[-pos_test:] + nwn_neg[-pos_test:] + kotor1_neg[-pos_test:]
    )

    pos_sents_test = persuade_sents[-pos_test:]
    print("neg_sents", len(neg_sents))
    print("neg_sents_test", len(neg_sents_test))
    print("neg_sents", len(neg_sents))
    print("pos_sents_test", len(pos_sents_test))
    print("pos_sents", len(pos_sents))

    source_neg = open("source_neg.txt", "w", encoding="utf-8")
    for element in neg_sents:
        source_neg.write(str(element) + "\n")

    source_persuade = open("source_persuade.txt", "w", encoding="utf-8")
    for element in pos_sents:
        source_persuade.write(str(element) + "\n")

    source_neg_test = open("source_neg_test.txt", "w", encoding="utf-8")
    for element in neg_sents_test:
        source_neg_test.write(str(element) + "\n")

    source_persuade_test = open("source_persuade_test.txt", "w", encoding="utf-8")
    for element in pos_sents_test:
        source_persuade_test.write(str(element) + "\n")


def compute_metrics(p):
    pred, labels = p
    pred = np.argmax(pred, axis=1)

    accuracy = accuracy_score(y_true=labels, y_pred=pred)
    recall = recall_score(y_true=labels, y_pred=pred)
    precision = precision_score(y_true=labels, y_pred=pred)
    f1 = f1_score(y_true=labels, y_pred=pred)

    return {"accuracy": accuracy, "precision": precision, "recall": recall, "f1": f1}


def finetune(langs, multiling):
    if langs == "it" and multiling == False:
        model = BertForSequenceClassification.from_pretrained(
            "dbmdz/bert-base-italian-cased"
        )
        tokenizer = BertTokenizer.from_pretrained("dbmdz/bert-base-italian-cased")
    if langs == "es" and multiling == False:
        model = BertForSequenceClassification.from_pretrained(
            "dccuchile/bert-base-spanish-wwm-cased"
        )
        tokenizer = BertTokenizer.from_pretrained(
            "dccuchile/bert-base-spanish-wwm-cased"
        )
    if langs == "de" and multiling == False:
        model = BertForSequenceClassification.from_pretrained("bert-base-german-cased")
        tokenizer = BertTokenizer.from_pretrained("bert-base-german-cased")
    if langs == "fr" and multiling == False:
        tokenizer = CamembertTokenizer.from_pretrained(
            "./camembert-base", output_hidden_states=True
        )
        model = CamembertForSequenceClassification.from_pretrained("./camembert-base")
    if langs == "en" and multiling == False:
        model = BertForSequenceClassification.from_pretrained("bert-base-cased")
        tokenizer = BertTokenizer.from_pretrained("bert-base-cased")
    if multiling:
        model = BertForSequenceClassification.from_pretrained(
            "bert-base-multilingual-cased"
        )
        tokenizer = BertTokenizer.from_pretrained("bert-base-multilingual-cased")

    source_neg = open_read("source_neg.txt").read().split("\n")
    source_pos = open_read("source_persuade.txt").read().split("\n")
    neg_labels = []
    pos_labels = []
    for i in range(len(source_neg)):
        neg_labels.append("no_persuade")
    for i in range(len(source_pos)):
        pos_labels.append("persuade")

    source = source_neg + source_pos
    labels = neg_labels + pos_labels

    train_batch, valid_batch, train_labels, valid_labels = train_test_split(
        source, labels, test_size=0.1
    )
    # train_batch = open_read("nmt/source_train.txt").read().split("\n")
    train_text = tokenizer(
        train_batch, return_tensors="pt", padding=True, truncation=True
    )
    # valid_batch = open_read("nmt/source_valid.txt").read().split("\n")
    # valid_batch = source[-int(len(source)*0.1):]
    valid_text = tokenizer(
        valid_batch, return_tensors="pt", padding=True, truncation=True
    )

    # train_labels = torch.tensor([int(x == "depression") for x in open_read("nmt/target_train.txt").read().split("\n")])
    # valid_labels = torch.tensor([int(x == "depression") for x in open_read("nmt/target_valid.txt").read().split("\n")])

    train_labels = torch.tensor([int(x == "persuade") for x in train_labels])
    valid_labels = torch.tensor([int(x == "persuade") for x in valid_labels])
    # test_labels = torch.tensor([int(x == "depression") for x in open_read("nmt/target_test.txt").read().split("\n")])

    train_dataset = IMDbDataset(train_text, train_labels)
    val_dataset = IMDbDataset(valid_text, valid_labels)
    # test_dataset = IMDbDataset(test_text, test_labels)

    training_args = TrainingArguments(
        output_dir="./results_pers",  # output directory
        evaluation_strategy="epoch",
        num_train_epochs=5,  # total number of training epochs
        per_device_train_batch_size=32,  # batch size per device during training
        per_device_eval_batch_size=64,  # batch size for evaluation
        warmup_steps=500,  # number of warmup steps for learning rate scheduler
        weight_decay=0.01,  # strength of weight decay
        logging_dir="./logs",  # directory for storing logs
        logging_steps=10,
        seed=seed,
    )

    trainer = Trainer(
        model=model,  # the instantiated ðŸ¤— Transformers model to be trained
        args=training_args,  # training arguments, defined above
        train_dataset=train_dataset,  # training dataset
        eval_dataset=val_dataset,  # evaluation dataset
        compute_metrics=compute_metrics,
    )

    trainer.train()
    if multiling:
        if type(langs) is list:
            trainer.save_model("multilang-finetuned-multilingual-persuade-bert")
        else:
            trainer.save_model("{}-finetuned-multilingual-persuade-bert".format(langs))
    else:
        trainer.save_model("{}-finetuned-persuade-bert".format(langs))


def _resulter(two_turds):
    if two_turds[0] > two_turds[1]:
        return 0
    else:
        return 1


def eval(shit_pile):
    cor = 0.0
    tot = 0.0
    labs = {
        "persuade": {"cor": 0.0, "err": 0.0},
        "no_persuade": {"cor": 0.0, "err": 0.0},
    }
    ls = ["no_persuade", "persuade"]
    for item in shit_pile:
        p = _resulter(item[0])
        c = item[1]

        if p == c:
            cor += 1
            labs[ls[c]]["cor"] += 1
        else:
            labs[ls[c]]["err"] += 1
        tot += 1
    print(cor, tot, cor / tot)
    for l, vs in labs.items():
        print(l, vs["cor"] / (vs["cor"] + vs["err"]))


def predict(langs, multiling):
    if langs == "it" and multiling == False:
        tokenizer = BertTokenizer.from_pretrained("dbmdz/bert-base-italian-cased")
    if langs == "es" and multiling == False:
        tokenizer = BertTokenizer.from_pretrained(
            "dccuchile/bert-base-spanish-wwm-cased"
        )
    if langs == "de" and multiling == False:
        tokenizer = BertTokenizer.from_pretrained("bert-base-german-cased")
    if langs == "fr" and multiling == False:
        tokenizer = CamembertTokenizer.from_pretrained(
            "./camembert-base", output_hidden_states=True
        )
    if langs == "en" and multiling == False:
        tokenizer = BertTokenizer.from_pretrained("bert-base-cased")
    if langs == "it" and multiling == True:
        tokenizer = BertTokenizer.from_pretrained("dbmdz/bert-base-italian-cased")
    if langs == "es" and multiling == True:
        tokenizer = BertTokenizer.from_pretrained(
            "dccuchile/bert-base-spanish-wwm-cased"
        )
    if langs == "de" and multiling == True:
        tokenizer = BertTokenizer.from_pretrained("bert-base-german-cased")
    if langs == "fr" and multiling == True:
        tokenizer = CamembertTokenizer.from_pretrained(
            "./camembert-base", output_hidden_states=True
        )
        model = "{}-finetuned-persuade-bert".format(langs)
    if langs == "en" and multiling == True:
        tokenizer = BertTokenizer.from_pretrained("bert-base-cased")
    if multiling == True:
        model = "{}-finetuned-multilingual-persuade-bert".format(langs)
    if multiling == False:
        model = "{}-finetuned-persuade-bert".format(langs)
    if type(langs) is list:
        tokenizer = BertTokenizer.from_pretrained("bert-base-multilingual-cased")
        model = "multilang-finetuned-multilingual-persuade-bert"

    pos_test = open_read("source_persuade_test.txt").read().split("\n")
    neg_test = open_read("source_neg_test.txt").read().split("\n")

    pos_test_labels = []
    neg_test_labels = []
    for i in range(len(pos_test)):
        pos_test_labels.append("persuade")
    for i in range(len(neg_test)):
        neg_test_labels.append("no_persuade")

    test_source = neg_test + pos_test
    test_labels = neg_test_labels + pos_test_labels

    test_text = tokenizer(
        test_source, return_tensors="pt", padding=True, truncation=True
    )
    test_labels = torch.tensor([int(x == "persuade") for x in test_labels])
    test_dataset = IMDbDataset(test_text, test_labels)
    model_path = str(model)
    if type(langs) is str:
        if langs == "fr" and "multilingual" not in model:
            model = CamembertForSequenceClassification.from_pretrained(model)
        else:
            model = BertForSequenceClassification.from_pretrained(model)
    else:
        model = BertForSequenceClassification.from_pretrained(model)
    training_args = TrainingArguments(
        output_dir="./results_pers",  # output directory
        evaluation_strategy="epoch",
        num_train_epochs=3,  # total number of training epochs
        per_device_train_batch_size=32,  # batch size per device during training
        per_device_eval_batch_size=64,  # batch size for evaluation
        warmup_steps=500,  # number of warmup steps for learning rate scheduler
        weight_decay=0.01,  # strength of weight decay
        logging_dir="./logs",  # directory for storing logs
        logging_steps=10,
    )
    trainer = Trainer(
        model=model,  # the instantiated ðŸ¤— Transformers model to be trained
        args=training_args,  # training arguments, defined above
    )
    model.eval()

    ps = trainer.predict(test_dataset)
    softshit = torch.nn.Softmax()
    preds = softshit(torch.from_numpy(ps.predictions)).numpy()
    pred_labels = []
    for pred in preds:
        pred_labels.append(np.argmax(pred))
    df = pd.DataFrame({"sents": test_source, "gold": test_labels, "preds": pred_labels})
    df.to_csv(model_path + ".csv")
    eval(list(zip(preds.tolist(), ps.label_ids.tolist())))


langs = [
    "en",
    "es",
    "it",
    "de",
    "fr",
]
for lang in langs:
    print("Training", lang, "monolingual model")
    get_traintest_data(lang)
    finetune(lang, multiling=False)
    predict(lang, multiling=False)
    print("Training", lang, "monolingual multilingual BERT model")
    finetune(lang, multiling=True)
    predict(lang, multiling=True)

get_traintest_data(langs)
finetune(langs, multiling=True)
predict(langs, multiling=True)
